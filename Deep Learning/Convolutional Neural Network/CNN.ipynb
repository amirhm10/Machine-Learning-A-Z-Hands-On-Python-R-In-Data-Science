{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining Data Set\n",
    "\n",
    "in the folder of dataset for better usage of Keras we split train and test set, again in the train set we spli dogs and cats set.\n",
    "10000 pictures we have here. 8000 for training and 2000 for test. 4000 dogs pictures and 4000 cats pictures in seperated folder with labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "`Sequential`: Sequence of layers , Convolutional Neural network is still a sequence of layers.\n",
    "\n",
    "`Convolution2D`: for the covolution operation and 2D for images because images are 2D.\n",
    "\n",
    "`MaxPooling2D`: for pooling step\n",
    "\n",
    "`Flatten`: for flatting pooled features\n",
    "\n",
    "`Dense`: use for add fully connected hidden layer in ann part of cnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1: Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Convolutional layer\n",
    "\n",
    "`1`- filters for number of features and kernel_size for size of every features.\n",
    "\n",
    "`2`- input_shape: for input shape. this convert all the images in the same format. (3 for rgb or 3 channels,256,256 dimensions of each channel) we want to use smaller format for better cpu. because we use TensorFlow backend the order will be like this (64,64,3) if we useTheano backend order will be (3,64,64)\n",
    "\n",
    "`3`- Activation Function: We use Rectifier for non-linearity and making sure that we don't have negative pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Convolution2D(filters=32,kernel_size=(3,3),input_shape=(64,64,3), activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Max Pooling Layer\n",
    "\n",
    "Turn Feature map to pooled feture maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Flattening Layer\n",
    "\n",
    "Taking Polling Layer and turn them into the input layer to ANN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Connection Step\n",
    "\n",
    "Build an ANN with fully connected hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(128,activation=\"relu\"))\n",
    "classifier.add(Dense(1,activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling The CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image data generator\n",
    "\n",
    "Generate batches of tensor image data with real-time data augmentation.\n",
    "\n",
    "The data will be looped over (in batches).\n",
    "\n",
    "this is because even 8000 images are not enough and we should use this class to build data like flipped or blur and so on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size=(64, 64),\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size=(64, 64),\n",
    "                                            batch_size=32,\n",
    "                                            class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train The CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 41s 162ms/step - loss: 0.5904 - accuracy: 0.6812 - val_loss: 0.5737 - val_accuracy: 0.7072\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 40s 162ms/step - loss: 0.5718 - accuracy: 0.7030 - val_loss: 0.5874 - val_accuracy: 0.6966\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 41s 166ms/step - loss: 0.5422 - accuracy: 0.7203 - val_loss: 0.5935 - val_accuracy: 0.6966\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 0.5373 - accuracy: 0.7271 - val_loss: 0.5669 - val_accuracy: 0.7026\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 0.5202 - accuracy: 0.7394 - val_loss: 0.5834 - val_accuracy: 0.7102\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 0.5139 - accuracy: 0.7419 - val_loss: 0.5830 - val_accuracy: 0.7132\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 0.4992 - accuracy: 0.7511 - val_loss: 0.5633 - val_accuracy: 0.7167\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 0.4834 - accuracy: 0.7640 - val_loss: 0.5246 - val_accuracy: 0.7465\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 0.4771 - accuracy: 0.7667 - val_loss: 0.5160 - val_accuracy: 0.7601\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 0.4576 - accuracy: 0.7841 - val_loss: 0.5424 - val_accuracy: 0.7460\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 0.4555 - accuracy: 0.7830 - val_loss: 0.5257 - val_accuracy: 0.7651\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 0.4375 - accuracy: 0.7884 - val_loss: 0.5624 - val_accuracy: 0.7359\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 0.4330 - accuracy: 0.7954 - val_loss: 0.5490 - val_accuracy: 0.7666\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 38s 154ms/step - loss: 0.4234 - accuracy: 0.8041 - val_loss: 0.5250 - val_accuracy: 0.7616\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 0.4234 - accuracy: 0.7984 - val_loss: 0.5149 - val_accuracy: 0.7671\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 0.4034 - accuracy: 0.8145 - val_loss: 0.5605 - val_accuracy: 0.7490\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 38s 154ms/step - loss: 0.3919 - accuracy: 0.8180 - val_loss: 0.6068 - val_accuracy: 0.7525\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 0.3860 - accuracy: 0.8242 - val_loss: 0.5439 - val_accuracy: 0.7631\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 38s 154ms/step - loss: 0.3653 - accuracy: 0.8328 - val_loss: 0.5370 - val_accuracy: 0.7772\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 0.3557 - accuracy: 0.8389 - val_loss: 0.5578 - val_accuracy: 0.7767\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 38s 154ms/step - loss: 0.3521 - accuracy: 0.8420 - val_loss: 0.5935 - val_accuracy: 0.7641\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 38s 154ms/step - loss: 0.3327 - accuracy: 0.8540 - val_loss: 0.5590 - val_accuracy: 0.7591\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 0.3326 - accuracy: 0.8518 - val_loss: 0.5634 - val_accuracy: 0.7732\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 39s 158ms/step - loss: 0.3181 - accuracy: 0.8600 - val_loss: 0.6005 - val_accuracy: 0.7686\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 0.3136 - accuracy: 0.8636 - val_loss: 0.5713 - val_accuracy: 0.7853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b9e05e5a60>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit_generator(training_set,\n",
    "               steps_per_epoch=250,\n",
    "               epochs=25,\n",
    "               validation_data=test_set,\n",
    "               validation_steps=62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing The accuracy on the test set\n",
    "\n",
    "out of expriment adding another convolutional layer will improve our accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.6826 - accuracy: 0.5497 - val_loss: 0.6658 - val_accuracy: 0.5912\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.6188 - accuracy: 0.6555 - val_loss: 0.5574 - val_accuracy: 0.7193\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 0.5708 - accuracy: 0.7036 - val_loss: 0.5627 - val_accuracy: 0.7268\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 45s 179ms/step - loss: 0.5437 - accuracy: 0.7254 - val_loss: 0.5346 - val_accuracy: 0.7314\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 44s 177ms/step - loss: 0.5117 - accuracy: 0.7405 - val_loss: 0.4858 - val_accuracy: 0.7656\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 44s 177ms/step - loss: 0.4939 - accuracy: 0.7560 - val_loss: 0.4647 - val_accuracy: 0.7782\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.4670 - accuracy: 0.7784 - val_loss: 0.4681 - val_accuracy: 0.7818\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 41s 164ms/step - loss: 0.4527 - accuracy: 0.7840 - val_loss: 0.4573 - val_accuracy: 0.7818\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 0.4343 - accuracy: 0.7905 - val_loss: 0.4498 - val_accuracy: 0.7939\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 0.4194 - accuracy: 0.8049 - val_loss: 0.4270 - val_accuracy: 0.7979\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 44s 174ms/step - loss: 0.4079 - accuracy: 0.8111 - val_loss: 0.4274 - val_accuracy: 0.8034\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.3945 - accuracy: 0.8194 - val_loss: 0.4729 - val_accuracy: 0.7656\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 44s 174ms/step - loss: 0.3816 - accuracy: 0.8303 - val_loss: 0.4539 - val_accuracy: 0.7858\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 45s 179ms/step - loss: 0.3698 - accuracy: 0.8324 - val_loss: 0.3929 - val_accuracy: 0.8191\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 45s 178ms/step - loss: 0.3604 - accuracy: 0.8334 - val_loss: 0.3963 - val_accuracy: 0.8170\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 44s 178ms/step - loss: 0.3353 - accuracy: 0.8501 - val_loss: 0.4147 - val_accuracy: 0.8120\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 42s 167ms/step - loss: 0.3244 - accuracy: 0.8550 - val_loss: 0.4117 - val_accuracy: 0.8206\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 40s 160ms/step - loss: 0.3169 - accuracy: 0.8577 - val_loss: 0.4118 - val_accuracy: 0.8266\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 40s 162ms/step - loss: 0.3088 - accuracy: 0.8670 - val_loss: 0.3899 - val_accuracy: 0.8342\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 0.2969 - accuracy: 0.8710 - val_loss: 0.3788 - val_accuracy: 0.8473\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 0.2844 - accuracy: 0.8784 - val_loss: 0.4029 - val_accuracy: 0.8407\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 40s 160ms/step - loss: 0.2708 - accuracy: 0.8850 - val_loss: 0.3779 - val_accuracy: 0.8397\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 40s 162ms/step - loss: 0.2590 - accuracy: 0.8890 - val_loss: 0.4128 - val_accuracy: 0.8221\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 0.2522 - accuracy: 0.8923 - val_loss: 0.4334 - val_accuracy: 0.8196\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 40s 162ms/step - loss: 0.2373 - accuracy: 0.9010 - val_loss: 0.4381 - val_accuracy: 0.8211\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b9e0de46d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Start The CNN\n",
    "classifier=Sequential()\n",
    "\n",
    "#Adding The Convolutional Layer\n",
    "classifier.add(Convolution2D(filters=32,kernel_size=(3,3),input_shape=(64,64,3), activation=\"relu\"))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(Convolution2D(filters=32,kernel_size=(3,3), activation=\"relu\"))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(Convolution2D(filters=64,kernel_size=(3,3), activation=\"relu\"))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Add The Flatten Layer\n",
    "classifier.add(Flatten())\n",
    "\n",
    "#Add Full Connection Hidden Layer and output layer\n",
    "classifier.add(Dense(128,activation=\"relu\"))\n",
    "classifier.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "#Compile the CNN\n",
    "classifier.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "#Generated Data already exist\n",
    "\n",
    "#Train The model\n",
    "classifier.fit(training_set,\n",
    "               steps_per_epoch=250,\n",
    "               epochs=25,\n",
    "               validation_data=test_set,\n",
    "               validation_steps=62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating The CNN with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 0.6903 - accuracy: 0.6094 - val_loss: 0.6210 - val_accuracy: 0.6855\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 0.5948 - accuracy: 0.6867 - val_loss: 0.5745 - val_accuracy: 0.7021\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 42s 166ms/step - loss: 0.5587 - accuracy: 0.7143 - val_loss: 0.5616 - val_accuracy: 0.7167\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 0.5407 - accuracy: 0.7232 - val_loss: 0.5208 - val_accuracy: 0.7374\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 42s 167ms/step - loss: 0.5222 - accuracy: 0.7415 - val_loss: 0.5551 - val_accuracy: 0.7248\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.5170 - accuracy: 0.7446 - val_loss: 0.5147 - val_accuracy: 0.7429\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 41s 162ms/step - loss: 0.5115 - accuracy: 0.7450 - val_loss: 0.5287 - val_accuracy: 0.7535\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 40s 158ms/step - loss: 0.4976 - accuracy: 0.7529 - val_loss: 0.6797 - val_accuracy: 0.6668\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 38s 152ms/step - loss: 0.4883 - accuracy: 0.7615 - val_loss: 0.5400 - val_accuracy: 0.7470\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 37s 150ms/step - loss: 0.4778 - accuracy: 0.7735 - val_loss: 0.5230 - val_accuracy: 0.7500\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 40s 158ms/step - loss: 0.4674 - accuracy: 0.7744 - val_loss: 0.5208 - val_accuracy: 0.7550\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 41s 164ms/step - loss: 0.4587 - accuracy: 0.7797 - val_loss: 0.5104 - val_accuracy: 0.7656\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 0.4602 - accuracy: 0.7799 - val_loss: 0.5221 - val_accuracy: 0.7434\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 0.4420 - accuracy: 0.7933 - val_loss: 0.5181 - val_accuracy: 0.7566\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 38s 154ms/step - loss: 0.4497 - accuracy: 0.7871 - val_loss: 0.5524 - val_accuracy: 0.7550\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 39s 154ms/step - loss: 0.4426 - accuracy: 0.7880 - val_loss: 0.5145 - val_accuracy: 0.7661\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 0.4365 - accuracy: 0.7952 - val_loss: 0.5270 - val_accuracy: 0.7636\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 0.4229 - accuracy: 0.8009 - val_loss: 0.5424 - val_accuracy: 0.7631\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 0.4268 - accuracy: 0.8031 - val_loss: 0.5332 - val_accuracy: 0.7626\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 40s 158ms/step - loss: 0.4130 - accuracy: 0.8100 - val_loss: 0.5732 - val_accuracy: 0.7641\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 0.4031 - accuracy: 0.8158 - val_loss: 0.5978 - val_accuracy: 0.7530\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 38s 153ms/step - loss: 0.4038 - accuracy: 0.8130 - val_loss: 0.5564 - val_accuracy: 0.7581\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 38s 151ms/step - loss: 0.3986 - accuracy: 0.8154 - val_loss: 0.5411 - val_accuracy: 0.7697\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 38s 151ms/step - loss: 0.3969 - accuracy: 0.8167 - val_loss: 0.6074 - val_accuracy: 0.7525\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 0.3905 - accuracy: 0.8221 - val_loss: 0.5358 - val_accuracy: 0.7828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b9e1163730>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "cnn=tf.keras.models.Sequential()\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=(3,3),\n",
    "                               activation=\"relu\",input_shape=(64,64,3)))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "cnn.add(tf.keras.layers.Dense(128,activation=\"relu\"))\n",
    "cnn.add(tf.keras.layers.Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "cnn.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "cnn.fit(training_set,steps_per_epoch=250,epochs=25,\n",
    "        validation_data=test_set,\n",
    "        validation_steps=62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Save and Load our deep learning models\n",
    "\n",
    "Saving a Keras model:\n",
    "\n",
    "\n",
    "model = ...  # Get model (Sequential, Functional Model, or Model subclass)\n",
    "\n",
    "model.save('path/to/location')\n",
    "\n",
    "Loading the model back:\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.load_model('path/to/location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hamed\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    }
   ],
   "source": [
    "classifier.save(\"model\")\n",
    "\n",
    "import keras\n",
    "\n",
    "model=keras.models.load_model(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing The ANN and CNN\n",
    "\n",
    "pip install graphviz\n",
    "\n",
    "pip install ann_visualizer\n",
    "\n",
    "from ann_visualizer.visualize import ann_viz;\n",
    "\n",
    "ann_viz(model, title=\"My first neural network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ann_visualizer.visualize import ann_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute ['dot', '-Tpdf', '-O', 'network.gv'], make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\graphviz\\backend.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(cmd, input, capture_output, check, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m         \u001b[0mproc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstartupinfo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_startupinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 854\u001b[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[0;32m    855\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1306\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1307\u001b[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001b[0m\u001b[0;32m   1308\u001b[0m                                          \u001b[1;31m# no special security\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-90fe2571525d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mann_viz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ann_visualizer\\visualize.py\u001b[0m in \u001b[0;36mann_viz\u001b[1;34m(model, view, filename, title)\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_attr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrowhead\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"none\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"#707070\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mview\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m             \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\graphviz\\files.py\u001b[0m in \u001b[0;36mview\u001b[1;34m(self, filename, directory, cleanup, quiet, quiet_view)\u001b[0m\n\u001b[0;32m    242\u001b[0m             \u001b[0mto\u001b[0m \u001b[0mretrieve\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mapplication\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mexit\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \"\"\"\n\u001b[1;32m--> 244\u001b[1;33m         return self.render(filename=filename, directory=directory,\n\u001b[0m\u001b[0;32m    245\u001b[0m                            \u001b[0mview\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m                            quiet=quiet, quiet_view=quiet_view)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\graphviz\\files.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, filename, directory, view, cleanup, format, renderer, formatter, quiet, quiet_view)\u001b[0m\n\u001b[0;32m    205\u001b[0m             \u001b[0mformat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m         rendered = backend.render(self._engine, format, filepath,\n\u001b[0m\u001b[0;32m    208\u001b[0m                                   \u001b[0mrenderer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m                                   quiet=quiet)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\graphviz\\backend.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[0mcwd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m     \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcapture_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquiet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrendered\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\graphviz\\backend.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(cmd, input, capture_output, check, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mExecutableNotFound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mExecutableNotFound\u001b[0m: failed to execute ['dot', '-Tpdf', '-O', 'network.gv'], make sure the Graphviz executables are on your systems' PATH"
     ]
    }
   ],
   "source": [
    "ann_viz(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "model=keras.models.load_model(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
